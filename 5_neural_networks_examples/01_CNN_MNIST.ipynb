{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tktvdabC18-s"
      },
      "source": [
        "<div style=\"width: 100%; clear: both;\">\n",
        "<div style=\"float: left; width: 50%;\">\n",
        "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
        "</div>\n",
        "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.418 · Aprenentatge automàtic</p>\n",
        "<p style=\"margin: 0; text-align:right;\">Grau en Ciència de Dades Aplicada</p>\n",
        "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis de Informàtica, Multimèdia i Telecomunicació</p>\n",
        "</div>\n",
        "</div>\n",
        "<div style=\"width:100%;\">&nbsp;</div>\n",
        "\n",
        "\n",
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niJxVteL18-y"
      },
      "source": [
        "In this notebook we will learn how to implement a convolutional neural network (CNN) to recognize the digits of the MNIST reference dataset. We will use the Keras library for the implementation, compilation and training of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-wpEt018-z"
      },
      "source": [
        "\"**Note**: If you are running this in [a colab notebook](https://colab.research.google.com/), you should install the dependencies by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wxEI2MuD18-0"
      },
      "outputs": [],
      "source": [
        "# ! pip install -q matplotlib tensorflow numpy "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJG-7PGF18-0"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xicFSzH18-1"
      },
      "source": [
        "The following code imports the packages needed for this exercise and also reads the data we will use to train the neural network. The MNIST data set corresponds to images with handwriten digits from 0 to 9 with a resolution of 28x28 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhvETBzY18-2",
        "outputId": "08ded1ba-825f-4dd6-86d6-b8926f8bc2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check GPU runtime type... \n",
            "Change Runtype Type in top menu for GPU acceleration\n",
            " \"Runtime\" -> \"Change Runtime Type\" -> \"GPU\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Check GPU runtime type... ')\n",
        "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
        "  print('Change Runtype Type in top menu for GPU acceleration')\n",
        "  print(' \"Runtime\" -> \"Change Runtime Type\" -> \"GPU\"')\n",
        "else:\n",
        "  print('OK!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mS-cmCLP18-2"
      },
      "outputs": [],
      "source": [
        "# Download the MNIST dataset and do the train/test partition\n",
        "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "4aAtRuL-18-2",
        "outputId": "80aba5c1-b60a-4aaa-fc6b-12f895f3024f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-0.5, 27.5, 27.5, -0.5)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the first image of the training set\n",
        "first_image = x_train_orig[0]\n",
        "first_image = np.array(first_image, dtype='float')\n",
        "pixels = first_image.reshape((28, 28))\n",
        "plt.imshow(pixels, cmap='gray')\n",
        "plt.tight_layout()\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZY0I7HR18-3",
        "outputId": "451ede9e-b3ea-4b07-9207-8ba977330a50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the image size\n",
        "x_train_orig[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aszWIV0m18-5",
        "outputId": "f6fb052d-19f4-49f0-a3ad-987ea18ed648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the original train data: (60000, 28, 28)\n",
            "Shape of the original test data: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# check the size of the original data\n",
        "print(\"Shape of the original train data: {}\".format(x_train_orig.shape))\n",
        "print(\"Shape of the original test data: {}\".format(x_test_orig.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOpSBhE18-3"
      },
      "source": [
        "We have 60,000 images for training and 10,000 for testing. The image size is $28 \\times 28$ pixels and they have only one channel (greyscale images).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarcHsuK18-5"
      },
      "source": [
        "### Preparing the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w8_sQa318-5"
      },
      "source": [
        "We will apply a simple normalization of the data so that the pixel values are in the range (0,1). Although there exist python classes to apply this normalization, here we will use the simplest method, which is to divide the values by the maximum value to obtain values in the range between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ueiwYek18-5",
        "outputId": "f685f22f-eaed-45b2-87d8-3808dfd2d56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min. and max. values before normalization are 0 and 255.\n",
            "Min. and max. values after normalization are 0.0 and 1.0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Find the min and max values in the training set\n",
        "v_min = np.min(x_train_orig)\n",
        "v_max = np.max(x_train_orig)\n",
        "print(\"Min. and max. values before normalization are {} and {}.\".format(v_min, v_max))\n",
        "\n",
        "# Normalization\n",
        "x_train_orig = (x_train_orig - v_min) / (v_max - v_min)\n",
        "x_test_orig = (x_test_orig - v_min) / (v_max - v_min)\n",
        "\n",
        "print(\"Min. and max. values after normalization are {} and {}.\".format(np.min(x_train_orig), np.max(x_train_orig)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWukNrMX18-6"
      },
      "source": [
        "We will now adjust the shapes of the training and test data using 4 dimensions, as the last dimension must be 1 to indicate that the images are in grayscale (1 channel).\n",
        "\n",
        "We do this because a convolutional layer expects at its input a 4-dimensional tensor with shape $(B,W,H,C)$ where $B$ is the batch size, $W$ and $H$ are respectively the width and weight of our images, and $C$ is the number of channels of the images ($C=1$ for greyscale images, $C=3$ for RGB color images, etc.)\n",
        "\n",
        "Thus, we want our train and test data to have a shape of <code>(60000, 28, 28, 1)</code> and <code>(10000, 28, 28, 1)</code> respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDhJJX3R18-6",
        "outputId": "28d4a276-d74d-4b92-fc39-1d9cd025da52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the train data: (60000, 28, 28, 1)\n",
            "Shape of the test data: (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train_orig.reshape(60000,28,28,1)\n",
        "x_test = x_test_orig.reshape(10000,28,28,1)\n",
        "\n",
        "print(\"Shape of the train data: {}\".format(x_train.shape))\n",
        "print(\"Shape of the test data: {}\".format(x_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjKf2IH18-7"
      },
      "source": [
        "Finally, we will encode the values of the class labels as `one-hot` vectors. The aim of this action is to convert the class label data to the same format of the output of our neural network, so they are directly comparable. For example, the output vector for an image with a 5 (class_label=5) would be encoded as follows [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.].\n",
        "\n",
        "Notice that our target are one-hot vectors in a 10-dimensional space because we cave 10 classes (the numbers from 0 to 9).\n",
        "\n",
        "To make this task easier, we will use the `to_categorial` function of` Keras.utils`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nk-xDhY18-7",
        "outputId": "c9cbbf5c-5b7a-4b6a-a5a5-ec777957f20b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of y_train_orig is (60000,) and the value of y_train_orig[0] is 5\n",
            "Shape of y_train is (60000, 10) and the value of y_train[0] is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "y_train = to_categorical(y_train_orig)\n",
        "y_test = to_categorical(y_test_orig)\n",
        "\n",
        "print(\"Shape of y_train_orig is {} and the value of y_train_orig[0] is {}\".format(y_train_orig.shape, y_train_orig[0]))\n",
        "print(\"Shape of y_train is {} and the value of y_train[0] is {}\".format(y_train.shape, y_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyLPtyuE18-7"
      },
      "source": [
        "### Create the CNN model\n",
        "\n",
        "We will use a `Sequential` model from Keras, which is very intuitive and easy to use. These types of models allow you to build the model by adding layer by layer. Specifically:\n",
        "\n",
        "- The first layer we add will be a convolutional layer with the following properties:\n",
        "   - Number of kernels (neurons): 64\n",
        "   - Size of the kernels: 3x3\n",
        "   - Kernel activation: relu\n",
        "- A *Flatten* layer must then be added to connect the output of the convolutional layer with the input of a dense layer.\n",
        "- Finally, we add a dense output layer, which will therefore have as many neurons as classes (10). The activation of this last layer will be the *softmax* function. This way the final prediction of the model will be the class with the highest probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3_6lDd18-7"
      },
      "source": [
        "The following code implements the CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ind1Flc-18-7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/robert/22.418/.conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add each layer to the model\n",
        "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYgcQkW18-8"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "Once the model has been defined, it must be compiled prior to the training process. In this case we will use the `Adam` optimization algorithm, the `categorical_crossentropy` loss function and the `accuracy` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2LLkt5218-8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOvIRgNG18-8"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "At this point we can train the model. We will make the model see each image in the training set 10 different times (i.e. `epochs` = 10), set a` batch_size` value equal to 128, and use the test set to validate the process after each training epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yGqDf0w18-9",
        "outputId": "071e266d-0c7f-470e-c38e-5d47cb702b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "mfit = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=128, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIMOMKRh18--"
      },
      "source": [
        "### Plot the accuracy and loss over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVHjbv2A18--"
      },
      "source": [
        "Next, we create a function that will allow us to see the evolution of the *accuracy* and the *loss* values in both the training and test sets at every epoch of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb2ocuvo18--"
      },
      "outputs": [],
      "source": [
        "# plot accuracy and loss\n",
        "def plot_prediction(n_epochs, mfit):\n",
        "    N = n_epochs\n",
        "    plt.style.use(\"ggplot\")\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n",
        "    fig.suptitle('Training Loss and Accuracy')\n",
        "    \n",
        "    ax1.plot(np.arange(0, N), mfit.history[\"accuracy\"], label=\"train\")\n",
        "    ax1.plot(np.arange(0, N), mfit.history[\"val_accuracy\"], label=\"val\")\n",
        "    ax1.set_title(\"Accuracy\")\n",
        "    ax1.set_xlabel(\"Epoch #\")\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    \n",
        "    ax2.plot(np.arange(0, N), mfit.history[\"loss\"], label=\"train\")\n",
        "    ax2.plot(np.arange(0, N), mfit.history[\"val_loss\"], label=\"val\")\n",
        "    ax2.set_title(\"Loss\")\n",
        "    ax2.set_xlabel(\"Epoch #\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "xxD8_9D918--",
        "outputId": "22c7e86c-04ae-4c39-bc5e-4b3fe5537814"
      },
      "outputs": [],
      "source": [
        "plot_prediction(n_epochs, mfit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POZmLjEA4_Hf"
      },
      "source": [
        "We appreciate a clear overfitting of the model during training. Remember we can set an early stopping rule in keras to stop training as soon as the validation loss does not improve for some iterations.\n",
        "\n",
        "Nonetheless the final validation accuracy of this simple CNN model achieves 98.23%, which is very good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWPmXhk18--"
      },
      "source": [
        "### Model predictions\n",
        "\n",
        "Finally, we can use the trained model to predict a class for any given image. Let's see how to make predictions for the first four images of the test set and check whether the results are correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "JMecAtU618--",
        "outputId": "1caef0a5-f1b4-4ab0-be06-1f6d50374506"
      },
      "outputs": [],
      "source": [
        "# Get predictions for the first four images of the test set\n",
        "predictions = model.predict(x_test[:4])\n",
        "\n",
        "\n",
        "for i in range(predictions.shape[0]):\n",
        "    # Visualize the image and predicted vs. real class label\n",
        "    image = x_test[i]\n",
        "    pixels = image.reshape((28, 28))\n",
        "    ax = plt.subplot(1,4,i+1)\n",
        "    plt.tight_layout()\n",
        "    ax.imshow(pixels, cmap='gray')\n",
        "    # Check the predicted class value for those images\n",
        "    ax.set_title('Predicted={}'.format(np.argmax(predictions[i,:])),{'fontsize':12})\n",
        "    ax.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuzop7NV18--"
      },
      "source": [
        "### EXERCISE\n",
        "\n",
        "Create a new model with the following convolutional model with Dropout and train it on MNIST data. Once trained, test it and compare the results with the single-layer model. \n",
        "\n",
        "- First Layer: Convolutional Layer (CONV1):\n",
        "   - Number of kernels: 32\n",
        "   - Size of the kernels: 3x3\n",
        "   - Kernel activation: relu\n",
        "\n",
        "- Second Layer: Max Pooling Layer (POOL1):\n",
        "\n",
        "   - Pool size: 2x2\n",
        "    \n",
        "- Third Layer: Convolutional Layer (CONV2):\n",
        "   - Number of kernels: 64\n",
        "   - Size of the kernels: 3x3\n",
        "   - Kernel activation: relu\n",
        "        \n",
        "- Fourth Layer: Max Pooling Layer (POOL2):\n",
        "\n",
        "    - Pool size: 2x2\n",
        "    \n",
        "- Fifth Layer: Flatten layer(FLAT1)\n",
        "        \n",
        "- Sixth Layer: Dropout layer(DROP1):\n",
        "\n",
        "    - Droupout_rate: 0.5\n",
        "    \n",
        "- Seventh Layer: Dense layer(FC1):\n",
        "\n",
        "    - Number of output neurons: 10\n",
        "    - Activation function: softmax\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrLnF-sFXBAk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvhj73UX18--"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "01_CNN_MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
